\frametitle{Algorithm Implementation}
\begin{lstlisting}[language=Python]
def attention_mechanism(x, num_heads=8):
    """Multi-head self-attention implementation"""
    batch_size, seq_len, d_model = x.shape

    # Split into multiple heads
    head_dim = d_model // num_heads
    x_reshaped = x.view(batch_size, seq_len,
                       num_heads, head_dim)

    # Compute attention weights
    attention_weights = torch.softmax(
        torch.matmul(x_reshaped, x_reshaped.transpose(-2, -1))
        / math.sqrt(head_dim), dim=-1
    )

    return torch.matmul(attention_weights, x_reshaped)
\end{lstlisting}
